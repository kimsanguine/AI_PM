# AI 제품 Observability: PM의 새로운 슈퍼파워

## 왜 AI 제품은 프로덕션에서 실패하는가

전통적 소프트웨어는 예측 가능하게 실패한다. 버그가 있으면 항상 같은 방식으로 재현된다. 하지만 AI 제품은 다르다.

**전형적 실패 패턴:**
- "어제는 완벽했는데 오늘은 왜 안 되지?"
- 사용자 A는 만족하는데 사용자 B는 불만족
- 입력값이 조금 달라지면 결과가 완전히 다름

**AI의 까다로운 특성:**
AI 모델은 틀렸을 때도 맞는 것처럼 보인다. 이를 '우아한 실패(graceful failure)'라고 부른다. 오류가 명확한 에러 메시지로 나타나지 않고, 그럴듯하지만 잘못된 답변을 제공한다.

**핵심 문제:**
Observability 없이 AI를 프로덕션에 배포하는 것은 레이더 없이 비행기를 날리는 것과 같다. 뭔가 잘못되고 있다는 것을 감지할 수 없다.

## Evals vs Observability: 근본적으로 다른 개념

많은 팀이 두 가지를 혼동한다. 명확히 구분해야 한다.

| 구분 | Evals | Observability |
|------|-------|---------------|
| 타이밍 | 출시 전 | 출시 후 (운영 중) |
| 특성 | 정적 평가 | 동적 모니터링 |
| 데이터 | 테스트셋 | 실제 프로덕션 데이터 |
| 목표 | 검증 | 추적 및 알림 |

**왜 둘 다 필요한가:**
- Evals 없는 Observability = 기준 없는 모니터링 (뭐가 정상인지 모름)
- Observability 없는 Evals = 어둠 속 시험 (출시 후 상황을 모르는 채 배포)

## AI PM의 Observability 역량 레벨: Before & After

### Before (모호한 버그 리포트)
```
"AI가 오늘 뭔가 이상해"
"사용자들이 답변을 안 좋아한대"
"응답이 느린 것 같아"
```

엔지니어 입장에서는 대응 불가능한 리포트다.

### After (증거 기반 진단)
```
"AI가 50단어 초과 쿼리에서 컨텍스트 10개 문서 초과일 때
실패하며, 트래픽 피크 시간(오전 9-10시)에 기술적 기능을
환각한다. 해당 시간대에만 좋아요율이 62%에서 38%로 떨어진다."
```

이제 구체적인 대응이 가능하다. 프롬프트 수정, 문서 제한, 캐싱 등 실행 계획을 세울 수 있다.

**차이점:** 두 번째는 해결 가능하다.

## PM이 추적해야 하는 3가지 메트릭 카테고리

### 1. 성능 메트릭 (Performance)

시스템이 얼마나 빠르고 안정적인가.

- **응답 시간**: P50, P95, P99 (밀리초 단위)
  - P95 1초 이상 = 사용자 이탈 위험
- **처리량**: 토큰/초, 요청/초
- **에러율**: HTTP 5xx, 타임아웃, API 실패
- **가용성**: 99.9%, 99.95% 등

**PM 액션:**
- 응답시간 P99가 3초 이상이면 → 모델 최적화 또는 문서 수 제한 검토

### 2. 품질 메트릭 (Quality)

AI의 답변이 정확하고 사용자가 만족하는가.

- **환각률**: 사실성 검증 (0-100%)
  - 자동화: LLM 기반 팩트체크
  - 수동: 샘플 검토 (주 1회)
- **사용자 만족도**: 좋아요/싫어요 비율
  - 목표: 좋아요율 80% 이상 유지
- **재시도율**: 사용자가 같은 질문을 다시 하는 비율
  - 높으면 = 첫 답변이 불만족
- **관련성 점수**: 답변이 질문과 얼마나 관련있는가 (자동 평가)

**PM 액션:**
- 품질이 급락하면 → 최근 프롬프트 변경 롤백
- 특정 카테고리만 낮으면 → 그 영역 데이터 강화

### 3. 비즈니스 메트릭 (Business)

AI 투자가 실제로 돈과 성장을 가져오는가.

- **사용자별 토큰 소비량**: 누가 가장 비용을 유발하는가
- **기능별 사용률**: 어느 기능이 실제로 쓰이는가
- **LLM 비용 대비 수익**: $1 소비에 몇 달러 수익
- **중단율**: AI 기능을 사용하지 않는 사용자 비율

**PM 액션:**
- 토큰 소비량 급증 → 비용 최적화 필요
- 특정 기능 미사용 → 삭제 또는 개선 결정

## 핵심 도구 비교: 어떤 것을 선택할까

### Helicone — LLM API 프록시 + 비용 관리

**특징:**
- 설정 시간: 5분 이내
- 코드 변경: 1줄 (API 엔드포인트 변경)
- 실시간 비용/성능 대시보드
- 캐싱으로 30% 비용 절감 가능

**PM이 보는 화면:**
- 일일 LLM 비용 추이
- 응답시간 분포 (P50/P95/P99)
- 토큰 사용 TOP 10 사용자
- 에러율 트렌드

**추천 상황:** 빠르게 기본 모니터링을 시작하고 싶을 때

### LangSmith — 체인 레벨 추적 + 평가

**특징:**
- LangChain 생태계 전문 플랫폼
- AI 파이프라인의 각 단계별 추적 가능
  - 쿼리 개선 → 임베딩 → 문서 검색 → 프롬프트 작성 → LLM 호출 → 파싱
- 자동 평가 시스템 (환각 탐지, 관련성 점수)
- A/B 테스트 프레임워크 내장

**PM이 보는 화면:**
- 각 프롬프트 버전별 성능 비교
- 어느 단계에서 가장 많이 실패하는가
- 사용자 세그먼트별 성능 차이

**추천 상황:** 프롬프트 개선와 실험을 자주 하는 팀

### Arize AI — 프로덕션 모니터링 + 드리프트 감지

**특징:**
- 엔터프라이즈급 모니터링
- 사용자 입력 패턴 변화 추적
- 모델 드리프트 자동 감지 (성능 저하 원인 파악)
- 실시간 알림

**PM이 보는 화면:**
- "어제와 달라진 입력 패턴"
- "왜 성능이 떨어졌는가" 자동 분석
- 세그먼트별 성능 저하 원인

**추천 상황:** 안정성과 품질이 최우선인 대규모 서비스

## 단계별 도입 로드맵

### Phase 1 (1-2주): 기본 모니터링

**목표:** "무엇이 일어나고 있는가"를 보기

1. Helicone 설정
   - OpenAI/Claude API 엔드포인트 변경
   - 대시보드에서 실시간 트래픽 모니터링

2. 기본 메트릭 정의
   - 응답시간 (P50, P95, P99)
   - 에러율
   - 일일 토큰 소비량

3. 알림 설정
   - 에러율 5% 초과 시 Slack 알림
   - 응답시간 P99 > 3초일 때 알림
   - 일일 비용이 예산의 80%를 넘을 때 알림

**완료 기준:**
- 대시보드를 매일 확인할 수 있어야 함
- 문제 발생 시 5분 이내 감지

### Phase 2 (1개월): 품질 모니터링 추가

**목표:** "얼마나 잘하고 있는가"를 알기

1. LangSmith 통합
   - 프롬프트 성능 추적 시작
   - 사용자 피드백 수집 (좋아요/싫어요)

2. A/B 테스트 프레임워크
   - 프롬프트 A vs B 자동 비교
   - 통계적 유의성 판단

3. 품질 메트릭 정의
   - 환각률 (주 1회 샘플 검토로 측정)
   - 좋아요율 (자동 수집)
   - 재시도율

**완료 기준:**
- 프롬프트 변경 시 성능 영향을 정량적으로 볼 수 있음
- 사용자 만족도가 80% 이상 유지되는지 확인

### Phase 3 (2-3개월): 고도화

**목표:** "왜 일어나고 있는가"를 이해하고 예측하기

1. 환각 탐지 자동화
   - LLM 기반 팩트체크 (Claude로 자신의 답을 검증)
   - 환각이 감지되면 자동 알림

2. 비즈니스 메트릭 상관관계 분석
   - 토큰 소비 vs 사용자 유지율
   - 응답시간 vs 기능 채택률

3. 예측적 알림 시스템
   - 성능 저하 추세 사전 감지
   - 시즈널 패턴 분석

**완료 기준:**
- 버그가 사용자에게 영향을 미치기 전에 감지
- 성능 저하 원인을 자동으로 파악

## Claude Code로 Observability 실전 적용

Claude Code와 함께 관찰성을 설계하고 구현하는 워크플로우:

**예시 프롬프트:**
```
우리 AI 서비스의 Observability 대시보드를 설계해줄래?

현재 상황:
- OpenAI GPT-4 기반 고객 지원 챗봇
- 월 100만 API 호출
- 사용자 만족도 관련 데이터 없음

필요한 것:
1. 성능/품질/비즈니스 3개 카테고리의 핵심 메트릭 정의
2. Helicone + LangSmith 조합 구현 계획
3. 알림 규칙 설정 (어떤 메트릭에서 몇 % 떨어지면?)
4. 대시보드 디자인 (우리 팀이 매일 봐야 할 수치)
```

**Claude가 할 수 있는 것:**
- 메트릭 정의 + 데이터 수집 파이프라인 설계
- Helicone/LangSmith 설정 코드 생성
- 대시보드 대시보드 데이터 조회 쿼리 작성
- 모니터링 자동화 스크립트 (cron job)
- 알림 규칙의 임계값 설정 추천

## 데이터 프라이버시 고려사항

관찰성이 좋다고 모든 데이터를 수집하는 것은 아니다.

**수집해야 할 것 (메타데이터):**
- 응답시간, 토큰 수, 에러 코드
- 좋아요/싫어요 (피드백만)
- 입력 길이, 문서 수 등 수치 정보

**수집하면 안 될 것:**
- 사용자의 실제 질문 내용
- 사용자의 실제 개인정보
- AI의 전체 응답 (요약만)

**프라이버시 지킴이:**
1. 동의 기반 수집 (옵트인)
   - "사용 데이터 분석에 동의하시겠습니까?" 명시
2. 샘플링 전략
   - 일반 요청: 1-5% 샘플링
   - 에러: 100% 수집 (파악 필요)
3. 익명화
   - 사용자 ID 해싱
   - IP 주소 마스킹

## PM 역할 확장 포인트

Observability를 갖추면 PM의 역할이 완전히 달라진다.

**Before (Observability 없음):**
```
PM: "AI가 이상해요"
→ 엔지니어: "뭐가 이상한데?"
→ PM: "음... 뭔지 모르겠는데 이상해요"
→ 결과: 아무도 모르고 끝남
```

**After (Observability 있음):**
```
PM: "정확히 언제, 어떤 조건에서, 왜 실패하는지 알고 있어요.
   여기 증거입니다. 이걸 어떻게 해결할지 의견 줄래요?"
→ 엔지니어: "아, 이거 해결할 수 있어요!"
→ 결과: 구체적 액션 플랜 수립
```

**PM이 할 수 있는 것:**
1. 어떤 메트릭이 비즈니스에 가장 중요한가 결정
2. 알림 임계값 설정 (무엇이 정상이고 비정상인가)
3. 트레이드오프 판단 (비용 vs 품질)
4. 우선순위 결정 (어떤 문제부터 고칠까)

**PM이 못하는 것 (엔지니어 몫):**
- 도구 설정
- 데이터 수집 파이프라인
- 알고리즘 개선

## ROI 측정: Observability가 정말 값어치가 있을까

### 시간 절감

| 지표 | Before | After | 개선 |
|------|--------|-------|------|
| 버그 식별까지 | 1일 | 5분 | 96% ↓ |
| 버그 해결까지 | 3일 | 30분 | 94% ↓ |
| 배포 후 문제 감지 | 사용자 리포트 | 자동 알림 (1분) | 99% ↠ |

### 안정성 향상

| 지표 | Before | After | 개선 |
|------|--------|-------|------|
| 월 프로덕션 장애 | 5회 | 1회 | 80% ↓ |
| 평균 장애 지속시간 | 4시간 | 15분 | 94% ↓ |
| 사용자 영향받는 비율 | 30% | 5% | 83% ↓ |

### 비용 절감

| 항목 | 절감액 | 기간 |
|------|--------|------|
| LLM 비용 (캐싱 + 최적화) | 10-20% | 월 |
| 엔지니어 장애 대응 (시간) | 월 40시간 → 5시간 | 월 |
| 고객 만족도 향상으로 인한 이탈 감소 | 5% | 분기 |

**첫 달 ROI:** 설정에 8시간, 1개월 운영으로 약 30시간 절감 = 4배 회수

## 실습 과제: Level 2

당신의 AI 서비스를 관찰해야 한다.

**과제:**
1. 본인 AI 서비스의 핵심 메트릭 10개 정의
   - 예: 응답시간 P95, 에러율, 좋아요율, 환각률, 월 토큰 소비량...

2. 각 메트릭을 3개 카테고리로 분류
   - 성능 (Performance)
   - 품질 (Quality)
   - 비즈니스 (Business)

3. 각 메트릭의 정상/주의/경고 기준 설정
   - 예: 응답시간 P95
     - 정상: < 500ms
     - 주의: 500-1000ms
     - 경고: > 1000ms

4. Helicone 대시보드에서 첫 주 데이터 모니터링
   - 실제 사용 패턴 파악
   - 이상 신호 감지 연습

**완료 시:** 당신의 AI 제품을 통제하는 PM이 되어 있을 것이다.


---

> **🔄 Human-in-the-Loop 연결** ([2.6-human-in-the-loop.md](./2.6-human-in-the-loop.md))
>
> AI 옵저빌리티는 HITL 원칙의 "프로덕션 적용"입니다.
> - **AI 제품의 HITL**: 모델 품질 모니터링은 자동화, 품질 저하 시 PM이 개입하여 판단
> - **비용 vs 품질 트레이드오프**: AI가 비용 데이터를 보여주지만, "이 품질 수준이 고객에게 충분한가?"는 PM의 판단
> - **신뢰 기반 자율성**: 모델 성능이 안정적이면 모니터링 주기를 늘리고, 변동 시 조이는 것이 2.6의 "루프 진화" 그 자체
---

> **© 2026 김생근 (Sanguine Kim)** | AI Agent Lead & AI Tutor
> 본 자료는 [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) 라이선스를 따릅니다.
> 교육·학술 목적 자유 이용 가능 | 상업적 이용 시 별도 라이선스 필요
> 강의·기업 교육·상업적 활용 문의: kimsanguine@gmail.com
