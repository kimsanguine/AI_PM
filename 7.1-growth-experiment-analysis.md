# 5.1 실험 분석: 데이터로 의사결정하기

## A/B 테스트 분석의 중요성

PM의 의사결정은 **직관(Intuition)과 데이터(Data)**의 조합이어야 합니다. 실험 데이터 분석은:
- **주관적 편향 제거** (Confirmation Bias 극복)
- **정량적 근거 제시** (이해관계자 설득)
- **빠른 반복 의사결정** (1주 단위 선택)

Claude Code를 활용하면 PM이 직접 1차 분석을 수행하고, 데이터팀은 더 깊은 분석에 집중할 수 있습니다.

---

## 실전: 온보딩 A/B 테스트 분석

### 상황: 신규 유저 온보딩 개선

**테스트 목표**: 온보딩 Step 2의 카피 수정이 전환율을 개선하는가?

| 그룹 | 카피 버전 | 샘플 수 |
|------|----------|--------|
| 대조군 (Control) | 원래 표현 | 2,850명 |
| 실험군 (Treatment) | 간단하고 직관적 버전 | 2,874명 |

---

## PM의 분석 프로세스

### 1단계: 파일 준비 및 Claude에 업로드

CSV 파일 (`ab_test_results.csv`):

```csv
user_id,group,converted,days_to_conversion,device,region,signup_date
001,control,1,2,mobile,seoul,2026-02-10
002,treatment,1,3,web,seoul,2026-02-10
003,control,0,,mobile,busan,2026-02-10
004,treatment,1,1,web,seoul,2026-02-10
...
[총 5,724행]
```

### 2단계: Claude에 분석 요청

```
> @ab_test_results.csv 온보딩 A/B 테스트 결과를 분석해줘.

다음 4가지를 포함해주면 좋겠어:

1. 통계적 유의성
   - 각 그룹의 전환율 (%)
   - p-value와 95% 신뢰구간
   - 최소 검출 효과 (MDE) 달성 여부

2. 세그먼트별 성과 차이
   - 신규/복귀 유저별
   - 모바일/웹별
   - 지역별

3. 품질 지표 확인
   - Retention (7일, 30일)
   - Engagement (활동일)
   - 부작용이 없는지 확인

4. 최종 권고안
   - Ship / Iterate / Kill 중 하나
   - 조건부 권고 (예: 신규만 배포)

Claude: A/B 테스트를 분석합니다.
```

### Claude의 분석 결과

```
# A/B 테스트 분석 결과: 온보딩 카피 수정

분석 시점: 2026-02-21 (테스트 기간: 11일)
표본 크기: 5,724명 (충분함)

---

## 1. 통계적 유의성

### 전환율 비교

| 그룹 | 전환율 | 표본 | 95% CI |
|------|--------|------|--------|
| **대조군** | 34.2% (977/2,850) | 2,850 | [32.5%, 35.9%] |
| **실험군** | 41.7% (1,199/2,874) | 2,874 | [39.9%, 43.5%] |
| **차이** | +7.5pp | — | [+4.1pp, +10.9pp] |

### 통계적 검증

- **p-value**: 0.003 ✓ (< 0.05, 유의함)
- **효과 크기 (Effect Size)**: 17% 향상
- **신뢰도**: 99.7% (p < 0.003)
- **검정력 (Power)**: 94% (좋음)

**결론**: 이 차이는 우연의 확률이 0.3% 미만.
실험군의 개선이 **통계적으로 유의합니다.**

---

## 2. 세그먼트별 성과 분석

### 신규 vs 복귀 유저

| 세그먼트 | 대조군 | 실험군 | 차이 | p-value | 유의성 |
|----------|--------|--------|------|---------|--------|
| **신규 유저** | 28.1% (445/1,580) | 39.3% (592/1,505) | +11.2pp | 0.0001 | ✓✓ |
| **복귀 유저** | 45.6% (532/1,270) | 46.2% (607/1,369) | +0.6pp | 0.87 | ✗ |

→ **신규 유저에게만 효과가 있음!**
복귀 유저는 이미 높은 전환율이므로, 카피 개선의 여지가 적음.

### 모바일 vs 웹

| 디바이스 | 대조군 | 실험군 | 차이 | p-value | 유의성 |
|----------|--------|--------|------|---------|--------|
| **모바일** | 31.8% (568/1,784) | 39.2% (714/1,821) | +7.4pp | 0.002 | ✓ |
| **웹** | 37.5% (409/1,066) | 45.1% (485/1,053) | +7.6pp | 0.005 | ✓ |

→ 양쪽 모두 개선. 모바일이 더 큰 절대값 향상.

### 지역별 (상위 3개)

| 지역 | 대조군 | 실험군 | 차이 | 표본 | 유의성 |
|------|--------|--------|------|------|--------|
| **서울** | 35.2% | 42.8% | +7.6pp | 2,141 | ✓ |
| **부산** | 32.1% | 40.5% | +8.4pp | 1,456 | ✓ |
| **대전** | 33.9% | 41.2% | +7.3pp | 1,089 | ✓ |

→ 지역별 차이 없음. 전반적으로 효과 일관됨.

---

## 3. 품질 지표 확인

### Retention (7일, 30일)

| 지표 | 대조군 | 실험군 | 차이 | 평가 |
|------|--------|--------|------|------|
| **Day 7 Retention** | 42.3% | 43.1% | +0.8pp | 차이 없음 (좋음) |
| **Day 30 Retention** | 18.2% | 18.9% | +0.7pp | 개선 (미미) |

→ 부작용 없음. Retention 하락 없이 전환율만 개선.

### Engagement (평균 활동일)

| 그룹 | 평균 활동일 | 표준편차 |
|------|-----------|---------|
| 대조군 | 3.2일 | 2.1 |
| 실험군 | 3.4일 | 2.2 |

→ 미미한 개선. 유의하지 않음.

**결론**: 품질 지표 악화 없음. 전환율 개선 = 순수 이득.

---

## 4. ROI 분석 (비용 vs 이득)

### 월 규모 영향도 추정

기본 가정:
- 월 신규 유저: 15,000명
- 신규 유저 전환율 현재: 28.1%
- 기대 이득 (신규만): 11.2pp

| 시나리오 | 월 전환 유저 증가 | 월 수익 증가* | 연간 수익 증가* |
|---------|-----------------|-------------|--------------|
| 보수 (신규만) | +1,680 | +$16,800 | +$201,600 |
| 기준 (전체) | +2,850 | +$28,500 | +$342,000 |
| 낙관 (3개월 누적) | +8,550 | +$85,500 | +$1,026,000 |

*: $10 ARPU 기준

### 개발 비용 대비
- 개발 시간: 이미 완료
- 배포 비용: 거의 없음
- **ROI**: ∞ (개발 비용 이미 지출됨)

---

## 5. 최종 권고안

### 결정: ✓✓ SHIP (조건부)

#### 즉시 배포 조건
1. **신규 유저 대상 효과가 압도적** (+11.2pp, p < 0.0001)
2. **품질 지표 악화 없음** (Retention 유지)
3. **구현 비용 최소** (이미 개발 완료)

#### 배포 전 체크리스트
- [ ] 모바일 UX 최종 확인 (모바일에서 +7.4pp 개선)
- [ ] 복귀 유저 대상 추가 최적화 계획 수립
- [ ] 성공 메트릭 모니터링 대시보드 준비

#### 배포 후 다음 이터레이션
1. **복귀 유저 특화 최적화** (별도 테스트)
   - 복귀 유저 대상 다른 카피 시도
   - 기대 효과: +3~5pp

2. **Step 3, 4 카피 최적화**
   - 현재 테스트는 Step 2만 대상
   - 전체 온보딩 3단계 최적화 가능성

3. **사용자 만족도 조사**
   - NPS (Net Promoter Score) 변화 추적
   - 정성적 피드백 수집

---

## 분석 Insight 요약

✓ **명확한 승리**: 신규 유저 대상 +11.2pp 전환율 개선
✓ **일관된 효과**: 모든 세그먼트에서 긍정적 (복귀 제외)
✓ **부작용 없음**: Retention, Engagement 악화 없음
⚠ **주의점**: 복귀 유저 효과 미미 → 추가 전략 필요
💡 **기회**: 다른 온보딩 단계의 최적화 여지 높음

---

**주의사항**:
- 아직 11일 데이터. 14일(2주) 이상 계속 진행 권장
- 요일별 트렌드 변동 가능성 (주말 vs 평일)
- 외부 변수 (광고, 뉴스) 확인 필수
```

---

## PM의 의사결정 프로세스

### 5분 요약 (리더십층 보고)

```
온보딩 카피 A/B 테스트 결과

결론: 즉시 배포 권장

• 신규 유저 전환율: 28.1% → 39.3% (+11.2pp)
• 통계적 유의성: p = 0.003 (매우 유의)
• 부작용: 없음 (Retention 유지)
• 기대 월 수익 증가: $28,500 (~$342K/년)

다음 단계: 복귀 유저 대상 별도 최적화
```

### 팀 공유 (개발팀, 데이터팀)

```
1. 결과 공유 (20분)
   - 통계 결과, 세그먼트 분석
   - 권고안 설명

2. Q&A (10분)
   - 데이터팀: 추가 분석 필요 여부
   - 개발팀: 배포 복잡도 확인
   - 디자인팀: 다음 테스트 아이디어

3. 다음 이터레이션 계획 (10분)
   - 복귀 유저 최적화 테스트
   - 다른 온보딩 단계 테스트
```

---

## 실습 과제

### 레벨 1: 따라하기

**과제 1**: 최근 진행한 또는 과거의 A/B 테스트 데이터를 준비하세요.
- CSV 형식으로 최소 1,000명 이상의 데이터
- 최소 2개 그룹(Control, Treatment) 구성
- 기본 메트릭 (전환율, 클릭율, 체류시간 등)

**과제 2**: Claude Code로 1차 분석을 수행하세요.
- 통계적 유의성(p-value, 신뢰구간) 확인
- 세그먼트별 분석 (모바일/웹, 신규/기존 등)
- 품질 지표 확인 (부작용 없는지)

**과제 3**: Claude의 분석 결과를 팀에 공유하고 의사결정을 내리세요.
- 5분 요약본 작성 (리더십층 보고)
- "Ship / Iterate / Kill" 중 선택
- 의사결정 근거 명확히 하기

---

### 레벨 2: 변형하기

**과제 1**: 이 모듈의 A/B 테스트 분석을 "전환율"이 아닌 "리텐션"으로 바꿔보세요. 7일/14일/30일 리텐션을 동시에 분석해야 할 때 프롬프트를 어떻게 구성하나요?

**과제 2**: 통계적 유의성이 나오지 않았을 때를 가정하세요. Claude에게 "유의성이 없는데, 이 실험에서 우리가 배울 수 있는 것은 무엇인가?"를 물어보세요. 실패한 실험에서도 학습할 수 있다.

---

## PM 역할 확장 포인트

### BEFORE: 데이터팀 의존
```
PM이 분석 요청
  ↓
데이터팀이 SQL 쿼리, 분석 수행 (3~5일)
  ↓
결과 리포트 받음
  ↓
PM이 의사결정
  ↓
팀 공유

문제점: 분석 완료까지 3~5일 대기
```

---

### AFTER: AI 네이티브 PM
```
PM이 직접 Claude로 1차 분석 (1시간)
  ↓
통계 유의성, 세그먼트 분석 즉시 파악
  ↓
데이터팀과 심화 논의 (2시간)
  ↓
PM이 팀에 의사결정 전달 (즉시)

이점: 분석 시간 3~5일 → 1시간
   데이터팀은 이상 탐지, 심화 분석에 집중
```

**새로운 책임**: 1차 분석 수행 → 데이터 기반 의사결정 주도 → 팀 정렬

### 🤔 PM 딜레마

**상황**: A/B 테스트 결과, 실험군의 전환율이 +12% 상승했다 (p=0.01). 그런데 동시에 고객 지원 문의가 +35% 증가했다. Claude는 "통계적으로 유의미한 개선"이라고 분석했지만, CS팀은 "이대로 롤아웃하면 우리 팀이 감당 못 한다"고 한다.

지표는 명확하게 개선됐지만, 그 개선이 실제 사용자 경험을 악화시킬 수 있다는 신호다. 숫자만 보면 배포할 이유가 있지만, 조직의 현실은 더 복잡하다. 데이터가 "맞다"고 말해도 실제로 배포해도 되는지 판단하기 어렵다.

**생각해볼 질문:**
- 핵심 지표(전환율)와 부수 지표(CS 문의)가 상충할 때, PM은 무엇을 기준으로 판단해야 하는가? 데이터 드리븐 결정의 한계는 어디인가?
- "지표 개선"이 "사용자 경험 개선"과 같지 않은 경우를 어떻게 식별하는가? 그리고 이미 배포한 후에 문제를 발견했다면?
- CS 문의 증가가 일시적(학습 곡선, 새로운 기능에 대한 교육)인지 구조적(설계 결함, 혼란)인지를 어떻게 구분하는가? 이 판단을 내릴 때 몇 주를 기다려야 하는가?

---

## Impact 공식: 언제 배포를 결정하는가?

실험 결과를 정량화:

```
Monthly Impact = Users × Current Rate × Expected Lift × Value

예시:
- Users: 월 신규 15,000명
- Current Rate: 28.1%
- Expected Lift: 11.2pp (39.3% - 28.1%)
- Value: $10 ARPU

= 15,000 × 0.281 × 0.112 × $10
= $4,704 / 월

12개월 = $56,448

반복 가능성: 3회 테스트 × $56K = $169K/년
```

**배포 기준**:
- Impact > 개발 비용: ✓ 배포
- Impact > 0 + 유의성 있음: ✓ 배포 (고려)
- Impact > 0 + 유의성 없음: ? 반복 또는 보류
- Impact < 0: ✗ 보류 또는 방향 변경

---

**다음 학습**: 5.2절에서는 여러 KPI를 통합 관리하고 자동화하는 방법을 배웁니다.

---

## ⚠️ 이럴 때는 이렇게

실험 데이터 분석에서는 통계적, 논리적 함정들이 숨어있습니다. PM이 놓칠 수 있는 상황들과 대처 방법을 살펴봅시다.

### **상황 1: 통계적 유의성이 없는데 Claude가 "의미 있는 개선"이라고 주장하는 경우**

**증상:**
- Claude의 분석: "p-value가 0.15입니다"
- 그 다음 결론: "의미 있는 개선이 관찰되었습니다"
- 또는: "이 개선은 배포 가능 수준입니다"
- 하지만 PM은 통계학을 배웠을 때 "p < 0.05"가 기준이라고 알고 있음

**원인:**
- Claude가 "통계적 유의성 (p < 0.05)"과 "실무적 유의성 (큰 효과)"을 구분하지 못함
- Claude는 데이터 패턴을 보고 "의미 있어 보인다"고 판단할 수 있음
- 또는 PM이 처음부터 통계적 유의성을 강조하지 않았을 경우

**해결 방법:**

Claude에게 명확히 통계적 기준을 제시:

```bash
> 이 실험 결과를 다시 평가해줄 수 있어?
  배포 기준:
  - p-value < 0.05 (필수)
  - 최소 2주 이상 테스트 데이터
  - 샘플 크기 >= 1000명 (그룹당)

  현재 결과가 이 기준을 모두 만족하는가?
```

만약 p-value가 0.15라면:

```bash
> p = 0.15라는 건 "우연의 확률이 15%"라는 뜻이지?
  이건 유의하지 않은 거 맞아.
  그럼 이 실험을 계속해야 할까, 멈춰야 할까?
  추가 표본을 모으면 유의성이 나올 가능성은?
```

**PM의 판단:**

통계 검정의 의미 이해:
- p-value < 0.05 = "이 결과가 우연일 확률이 5% 미만"
- p-value = 0.15 = "이 결과가 우연일 확률이 15%" → 배포하기엔 위험
- 따라서 유의성이 없으면 "더 데이터를 모으거나" "다시 테스트하거나" "이 아이디어는 포기"

**흔한 실수:**
- "효과가 작더라도 p < 0.05만 만족하면 배포" → 옳음
- "효과가 크지만 p > 0.05면 우연일 수 있으니까 배포 안 함" → 옳음
- "효과가 크고 p = 0.07이니까 거의 유의하니까 배포" → 틀림 (p-hacking의 위험)

---

### **상황 2: Simpson's Paradox - 전체 데이터는 A가 우세하지만 세그먼트별로는 B가 우세한 경우**

**증상:**
- Claude의 결론: "전체적으로 A의 성과가 더 좋습니다 (55% vs 45%)"
- 하지만 PM이 "기존 사용자 중에는 어떻게 되나?" 확인해보니 B가 우세
- 또는 "모바일과 웹을 각각 보니까 완전히 다르네?"
- "어? 전체로는 A가 우세한데, 왜 세부 그룹에선 B가 우세야?"

**원인:**
- 각 세그먼트의 규모가 매우 다를 때 발생하는 통계적 역설
- 예시:
  - 신규 사용자: A=50%, B=40% (신규 1000명)
  - 기존 사용자: A=30%, B=70% (기존 100명)
  - 전체: A=48%, B=52% (신규가 많아서 A의 절대값이 높음)
- Claude는 전체 데이터만 보면 "A가 우세"라고 결론 내림

**해결 방법:**

Claude에게 명시적으로 세그먼트별 분석 요청:

```bash
> 이 A/B 테스트 결과를 세그먼트별로 다시 분석해줄 수 있어?

  특히:
  1. 신규 사용자 vs 기존 사용자 (각각 A vs B 비교)
  2. 모바일 vs 웹
  3. 가입 후 1개월 이내 vs 1개월 이상

  "전체에서는 A가 우세하지만,
   어떤 세그먼트에서는 B가 우세한가?" 를 찾아줘
```

만약 세그먼트별 역설이 발견되면:

```bash
> 신규에선 A가 우세(50% vs 40%)하고
  기존에선 B가 우세(30% vs 70%)한데,
  이게 왜 일어났을까?

  각 세그먼트별로:
  - 표본 크기는?
  - 차이의 크기는?
  - 이런 역설이 진짜 의미 있는 건가, 아니면 우연인가?
```

**PM의 판단:**

Simpson's Paradox 상황의 의사결정:
- 전체 승자가 아니라 **각 중요 세그먼트의 승자를 구분**
- 예: "신규 사용자만 A로 배포하고, 기존은 B 유지" 같은 세분화된 결정 가능
- 또는 "이 역설이 뭔가 중요한 신호일 수 있음" → 추가 조사 필요

**주의사항:**
- 세그먼트별 표본 크기가 작으면 역설이 관찰되지 않을 수 있음
- 세그먼트가 너무 많으면 p-hacking의 위험 (우연의 일치)
- 사전에 "이 세그먼트를 비교하겠다"고 정한 후 분석해야 함

---

### **상황 3: 외부 변수 오염 - 테스트 기간에 광고 캠페인, 뉴스, 계절 변화 등이 겹쳤을 때**

**증상:**
- Claude의 분석: "개선 효과: +5pp, p=0.003 (매우 유의)"
- PM이 생각해보니: "어, 그런데 이 기간에 마케팅팀이 대규모 광고 캠페인을 했었어"
- 또는: "설 명절(또는 여름휴가) 기간이 포함되어 있었나?"
- 또는: "경쟁사의 대규모 할인이 이 기간에 있었어"

**원인:**
- Claude는 **데이터만** 본다. 그 데이터 뒤의 실제 사건/변수는 모름
- 데이터 분석에서 "상관관계 ≠ 인과관계"의 전형적 예
- PM과 마케팅팀 간 커뮤니케이션 부족

**해결 방법:**

분석 결과를 받은 후 데이터팀/마케팅팀과 확인:

```bash
# 데이터팀/마케팅팀에게 물어보기
"우리가 [테스트 기간]에 진행한 다른 대규모 활동이 있었나?
- 광고 캠페인
- 콘텐츠/뉴스 출시
- PR 활동
- 경쟁사 주요 움직임"
```

만약 외부 변수가 있었다면 Claude에게 명시:

```bash
> 이 A/B 테스트 분석에서 한 가지 더 고려해야 할 게 있어.

  테스트 기간 [2026-02-10 ~ 2026-02-21]에
  마케팅팀이 대규모 광고 캠페인을 진행했어.

  그렇다면:
  1. 광고 노출 여부로 세그먼트 분석 가능한가?
  2. 실험군/대조군의 광고 노출 분포가 균등한가?
  3. 이 외부 변수를 제거하면 실제 효과는 몇 pp인가?
```

**PM의 예방 조치:**

실험을 시작할 때부터:

```bash
> A/B 테스트를 설계할 때 체크리스트:
  - 테스트 기간 동안 마케팅 활동: 있음/없음
  - 알려진 계절 변수: (예: 설명절, 여름)
  - 경쟁 상황 변화: 있음/없음

  이런 변수들이 있으면 "테스트 결과의 신뢰도 낮음"이라고
  사전에 기록하고 시작
```

**통계적 보정:**

만약 외부 변수를 식별했다면:

```bash
> 광고 노출 여부를 제어 변수로 해서
  A/B 효과를 다시 계산할 수 있을까?
  (ANCOVA: Analysis of Covariance)

  또는 광고 미노출 그룹만 따로 분석해줄 수 있어?
```

**PM의 최종 판단:**
- 통계적으로 유의한 결과도 외부 변수가 있으면 신뢰도가 떨어짐
- "완벽한 통제"는 불가능하지만, 알려진 변수는 최대한 문서화
- 외부 변수를 고려한 후에도 효과가 있으면 그제야 "진짜 개선"이라고 할 수 있음


---

> **🔄 Human-in-the-Loop 연결** ([2.6-human-in-the-loop.md](./2.6-human-in-the-loop.md))
>
> 실험 분석은 "Confidence Scoring" 패턴이 핵심입니다.
> - **통계적 유의성 ≠ 비즈니스 유의성**: AI가 p-value를 계산하지만, "이 개선이 투자 대비 가치 있는가?"는 PM이 판단 (→ 2.6 패턴 3)
> - **외부 변수 감지**: AI가 놓치는 "그 주에 마케팅 캠페인이 있었다"는 맥락을 PM이 추가 (→ 2.6 섹션 2.2)
> - **Go/No-Go 결정**: 불확실성 하에서의 결단은 AI가 아닌 PM의 영역 (→ 2.6 섹션 5.2)
---

> **© 2026 김생근 (Sanguine Kim)** | AI Agent Lead & AI Tutor
> 본 자료는 [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) 라이선스를 따릅니다.
> 교육·학술 목적 자유 이용 가능 | 상업적 이용 시 별도 라이선스 필요
> 강의·기업 교육·상업적 활용 문의: kimsanguine@gmail.com
